{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pnii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pnii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pnii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Télécharger les ressources nécessaires pour NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:\\\\Projo\\\\Projet-Data-IA\\\\Merged_all_tables\\\\merged_final.csv', sep=',')\n",
    "df.sort_values(by='averageRating', ascending=False, inplace=True)\n",
    "df.drop(columns=['poster_path', 'backdrop_path', 'nconst_director'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommandations pour Le parrain :\n",
      "                 title\n",
      "1189      Forrest Gump\n",
      "3729  Django Unchained\n",
      "2616      Interstellar\n",
      "4792          Parasite\n",
      "3288         Inception\n"
     ]
    }
   ],
   "source": [
    "# Initialiser le stemmer et le lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Fonction pour appliquer le stemming et la lemmatisation à un texte\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Charger les données (Assurez-vous que df contient une colonne 'overview' avant d'exécuter ce code)\n",
    "# df = pd.read_csv('votre_fichier.csv')\n",
    "\n",
    "# Appliquer la fonction de prétraitement à la colonne \"overview\"\n",
    "df['processed_overview'] = df['overview'].apply(preprocess_text)\n",
    "\n",
    "# Définir les colonnes catégorielles et numériques\n",
    "cat_cols = [\"title\", \"genre1\", \"genre2\", \"production_companies_name\", \"Director_name\", \"Actors_Actresses\", \"processed_overview\"]\n",
    "num_cols = [\"numVotes\", \"startYear\", \"runtimeMinutes\", \"popularity\"]\n",
    "\n",
    "# Créer une classe de transformation personnalisée pour extraire les sentiments de l'aperçu\n",
    "class SentimentExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        sentiments = []\n",
    "        for overview in X[\"processed_overview\"]:\n",
    "            sentiment = self.analyzer.polarity_scores(overview)\n",
    "            sentiments.append(list(sentiment.values()))\n",
    "        return sentiments\n",
    "\n",
    "# Créer un pipeline de transformation pour les caractéristiques\n",
    "feature_pipeline = FeatureUnion([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"sentiment\", SentimentExtractor())\n",
    "])\n",
    "\n",
    "# Appliquer le TfidfVectorizer sur la colonne \"processed_overview\"\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df[\"processed_overview\"])\n",
    "\n",
    "# Appliquer le SentimentExtractor sur la colonne \"processed_overview\"\n",
    "sentiment_extractor = SentimentExtractor()\n",
    "X_sentiment = sentiment_extractor.fit_transform(df)\n",
    "\n",
    "# Concaténer les résultats\n",
    "X_features = sp.hstack((X_tfidf, X_sentiment), format='csr')\n",
    "\n",
    "# Standardiser les colonnes numériques\n",
    "scaler = StandardScaler()\n",
    "X_numerical = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "# Spécifier les poids pour chaque colonne numérique\n",
    "num_feature_weights = {\n",
    "    \"numVotes\": 1,\n",
    "    \"startYear\": 1,\n",
    "    \"runtimeMinutes\": 1,\n",
    "    \"popularity\": 2,\n",
    "}\n",
    "\n",
    "# Appliquer la pondération aux caractéristiques numériques en utilisant les poids spécifiés\n",
    "X_numerical_weighted = X_numerical.copy()\n",
    "for col, weight in num_feature_weights.items():\n",
    "    col_index = num_cols.index(col)\n",
    "    X_numerical_weighted[:, col_index] *= weight\n",
    "\n",
    "# Créer un vecteur de poids pour les caractéristiques catégorielles\n",
    "cat_feature_weights = {\n",
    "    \"title\": 1,\n",
    "    \"genre1\": 3,\n",
    "    \"genre2\": 1,\n",
    "    \"production_companies_name\": 2,\n",
    "    \"Director_name\": 1,\n",
    "    \"Actors_Actresses\": 2,\n",
    "    \"processed_overview\": 2\n",
    "}\n",
    "\n",
    "# Créer un encodeur one-hot\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Adapter l'encodeur aux données d'entraînement et le transformer\n",
    "X_cat_encoded = encoder.fit_transform(df[cat_cols])\n",
    "\n",
    "# Appliquer la pondération aux caractéristiques catégorielles en utilisant les poids spécifiés\n",
    "for col, weight in cat_feature_weights.items():\n",
    "    col_index = cat_cols.index(col)\n",
    "    X_cat_encoded[:, col_index] *= weight\n",
    "\n",
    "# Concaténer les caractéristiques encodées et standardisées\n",
    "X = sp.hstack((X_features, X_numerical_weighted, X_cat_encoded), format='csr')\n",
    "\n",
    "# Calculer la similarité cosinus entre les films\n",
    "cosine_sim = cosine_similarity(X, X)\n",
    "\n",
    "# Fonction de recommandation de films similaires\n",
    "def recommend_movies(movie_title, cosine_sim=cosine_sim, df=df):\n",
    "    # Vectoriser les titres de films\n",
    "    title_vectorizer = TfidfVectorizer().fit_transform(df['title'])\n",
    "    # Calculer la similarité cosinus entre le titre donné et les titres dans le dataset\n",
    "    title_sim = cosine_similarity(title_vectorizer, title_vectorizer)\n",
    "    # Trouver l'index du titre le plus similaire au titre donné\n",
    "    title_idx = df[df['title'].str.lower() == movie_title.lower()].index\n",
    "    if len(title_idx) == 0:\n",
    "        # Si le titre exact n'est pas trouvé, utiliser la similarité cosinus pour trouver les titres les plus proches\n",
    "        title_similarities = cosine_similarity(TfidfVectorizer().fit_transform([movie_title]), title_vectorizer)\n",
    "        most_similar_title_idx = title_similarities.argsort()[0][-1]\n",
    "        title_idx = [most_similar_title_idx]\n",
    "\n",
    "    idx = title_idx[0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:6]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return df.iloc[movie_indices][['title']]  # Retourner les titres des films recommandés\n",
    "\n",
    "# Exemple d'utilisation : obtenir des recommandations pour un film spécifique\n",
    "movie_title = \"Le parrain\"\n",
    "recommended_movies = recommend_movies(movie_title)\n",
    "print(\"Recommandations pour\", movie_title, \":\")\n",
    "print(recommended_movies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

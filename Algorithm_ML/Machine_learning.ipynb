{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Pour la vectorisation TF-IDF\n",
    "from sklearn.metrics.pairwise import cosine_similarity       # Pour calculer la similarité cosinus\n",
    "from sklearn.preprocessing import StandardScaler             # Pour standardiser les caractéristiques numériques\n",
    "from sklearn.pipeline import FeatureUnion                    # Pour combiner les pipelines de caractéristiques\n",
    "from sklearn.base import BaseEstimator, TransformerMixin     # Pour créer une classe de transformation personnalisée\n",
    "from sklearn.preprocessing import OneHotEncoder              # Pour encoder les caractéristiques catégorielles\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer        # Pour l'analyse des sentiments\n",
    "import scipy.sparse as sp                                    # Pour gérer les matrices creuses\n",
    "import numpy as np                                           # Pour les opérations numériques\n",
    "import pandas as pd                                          # Pour manipuler les données\n",
    "import nltk                                                  # Pour le traitement du langage naturel\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrage basé sur le contenu :\n",
    "\n",
    "Utilise les caractéristiques des articles (ou des utilisateurs) pour trouver des recommandations similaires.\n",
    "* Avantages : Peut fournir des recommandations explicites et peut surmonter les problèmes de démarrage à froid.\n",
    "* Inconvénients : Peut manquer de sérendipité et de diversité dans les recommandations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:\\\\Projo\\\\Projet-Data-IA\\\\Merged_all_tables\\\\merged_final.csv', sep=',')\n",
    "df.sort_values(by='averageRating', ascending=False, inplace=True)\n",
    "df.drop(columns=['poster_path', 'backdrop_path', 'nconst_director'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Téléchargement des ressources nécessaires pour NLTK : \n",
    "* Les ressources requises pour la tokenisation, la lemmatisation et les mots vides (stopwords) sont téléchargées à partir de NLTK.\n",
    "\n",
    "2. Initialisation du stemmer et du lemmatizer : \n",
    "* Le stemmer (racinisateur) et le lemmatizer sont initialisés pour réduire les mots à leur base et obtenir les formes de mots canoniques respectivement.\n",
    "\n",
    "3. Fonction pour appliquer le stemming et la lemmatisation à un texte : \n",
    "* Une fonction preprocess_text est définie pour effectuer le prétraitement textuel, y compris la tokenisation, la suppression des mots vides, le stemming et la lemmatisation.\n",
    "\n",
    "4. Application de la fonction de prétraitement à la colonne \"overview\" : \n",
    "* La fonction preprocess_text est appliquée à la colonne \"overview\" du DataFrame pour prétraiter le texte des aperçus des films et stocker les résultats dans une nouvelle colonne appelée \"processed_overview\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PorterStemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialiser le stemmer et le lemmatizer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m \u001b[43mPorterStemmer\u001b[49m()  \u001b[38;5;66;03m# Initialiser le stemmer (racinisateur) pour réduire les mots à leur base\u001b[39;00m\n\u001b[0;32m      3\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()  \u001b[38;5;66;03m# Initialiser le lemmatizer pour obtenir les formes de mots canoniques\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Fonction pour appliquer le stemming et la lemmatisation à un texte\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PorterStemmer' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialiser le stemmer et le lemmatizer\n",
    "stemmer = PorterStemmer()  # Initialiser le stemmer (racinisateur) pour réduire les mots à leur base\n",
    "lemmatizer = WordNetLemmatizer()  # Initialiser le lemmatizer pour obtenir les formes de mots canoniques\n",
    "\n",
    "# Fonction pour appliquer le stemming et la lemmatisation à un texte\n",
    "def preprocess_text(text):\n",
    "    # Tokenization : Diviser le texte en mots (tokens)\n",
    "    words = word_tokenize(text)\n",
    "    # Supprimer les mots vides (stopwords) et mettre en minuscule\n",
    "    words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
    "    # Stemming : Réduire les mots à leur racine\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    # Lemmatization : Obtenir les formes canoniques des mots\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "    return ' '.join(lemmatized_words)  # Retourner le texte prétraité sous forme de chaîne de caractères\n",
    "\n",
    "# Appliquer la fonction de prétraitement à la colonne \"overview\"\n",
    "df['processed_overview'] = df['overview'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importation des bibliothèques : \n",
    "* Les bibliothèques nécessaires sont importées pour les différentes opérations de prétraitement des données et de recommandation de films.\n",
    "\n",
    "2. Définition des colonnes catégorielles et numériques : \n",
    "* Les colonnes pertinentes du DataFrame sont spécifiées pour l'analyse et le traitement ultérieurs.\n",
    "\n",
    "3. Création d'une classe de transformation personnalisée pour extraire les sentiments : \n",
    "* Une classe SentimentExtractor est créée pour extraire les sentiments des aperçus de films en utilisant l'analyseur de sentiment NLTK.\n",
    "\n",
    "4. Création d'un pipeline de transformation pour les caractéristiques : \n",
    "* Un pipeline est créé pour combiner la vectorisation TF-IDF des aperçus et l'extraction des sentiments.\n",
    "\n",
    "5. Application du TfidfVectorizer sur la colonne \"processed_overview\" : \n",
    "* Le TfidfVectorizer est appliqué pour convertir les aperçus en vecteurs TF-IDF.\n",
    "\n",
    "6. Application du SentimentExtractor sur la colonne \"processed_overview\" : \n",
    "* L'extracteur de sentiments est appliqué pour obtenir les scores de sentiment des aperçus.\n",
    "\n",
    "7. Concaténation des résultats : \n",
    "* Les vecteurs TF-IDF et les scores de sentiment sont concaténés pour obtenir les caractéristiques textuelles combinées.\n",
    "\n",
    "8. Standardisation des colonnes numériques : \n",
    "* Les caractéristiques numériques sont standardisées pour les mettre à la même échelle.\n",
    "\n",
    "9. Spécification des poids pour chaque colonne numérique : \n",
    "* Des poids sont spécifiés pour chaque colonne numérique en fonction de leur importance relative.\n",
    "\n",
    "10. Application de la pondération aux caractéristiques numériques : \n",
    "* Les poids spécifiés sont appliqués aux caractéristiques numériques.\n",
    "\n",
    "11. Création d'un vecteur de poids pour les caractéristiques catégorielles : \n",
    "* Des poids sont spécifiés pour chaque colonne catégorielle en fonction de leur importance relative.\n",
    "\n",
    "12. Création d'un encodeur one-hot et application de la pondération : \n",
    "* Les données catégorielles sont encodées en utilisant un encodeur one-hot et les poids spécifiés sont appliqués.\n",
    "\n",
    "13. Concaténation des caractéristiques encodées et standardisées : \n",
    "* Les caractéristiques textuelles, numériques et catégorielles sont concaténées pour former une seule matrice de caractéristiques.\n",
    "\n",
    "14. Calcul de la similarité cosinus entre les films : \n",
    "* La similarité cosinus est calculée entre les films à l'aide de la matrice de caractéristiques combinée.\n",
    "\n",
    "15. Fonction de recommandation de films similaires : \n",
    "* Une fonction est définie pour recommander des films similaires à un film donné en utilisant la similarité cosinus.\n",
    "\n",
    "16. Exemple d'utilisation : \n",
    "* Un exemple est donné pour obtenir des recommandations pour un film spécifique (\"Le Parrain\") et les résultats sont affichés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommandations pour Le Roi Lion :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        Forrest Gump\n",
       "1    Django Unchained\n",
       "2        Interstellar\n",
       "3            Parasite\n",
       "4           Inception\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Définir les colonnes catégorielles et numériques\n",
    "cat_cols = [\"title\", \"genre1\", \"genre2\", \"production_companies_name\", \"Director_name\", \"Actors_Actresses\", \"processed_overview\"]\n",
    "num_cols = [\"numVotes\", \"startYear\", \"runtimeMinutes\", \"popularity\"]\n",
    "\n",
    "# Créer une classe de transformation personnalisée pour extraire les sentiments de l'aperçu\n",
    "class SentimentExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()  # Initialiser l'analyseur de sentiment\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        sentiments = []\n",
    "        for overview in X[\"processed_overview\"]:\n",
    "            sentiment = self.analyzer.polarity_scores(overview)  # Calculer les scores de sentiment\n",
    "            sentiments.append(list(sentiment.values()))  # Ajouter les scores à la liste\n",
    "        return sentiments\n",
    "\n",
    "# Créer un pipeline de transformation pour les caractéristiques\n",
    "feature_pipeline = FeatureUnion([\n",
    "    (\"tfidf\", TfidfVectorizer()),  # Vectorisation TF-IDF pour les aperçus\n",
    "    (\"sentiment\", SentimentExtractor())  # Extraction de sentiments pour les aperçus\n",
    "])\n",
    "\n",
    "# Appliquer le TfidfVectorizer sur la colonne \"processed_overview\"\n",
    "tfidf_vectorizer = TfidfVectorizer()  # Initialiser le vectoriseur TF-IDF\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df[\"processed_overview\"])  # Appliquer sur les aperçus\n",
    "\n",
    "# Appliquer le SentimentExtractor sur la colonne \"processed_overview\"\n",
    "sentiment_extractor = SentimentExtractor()  # Initialiser l'extracteur de sentiments\n",
    "X_sentiment = sentiment_extractor.fit_transform(df)  # Appliquer sur les aperçus\n",
    "\n",
    "# Concaténer les résultats\n",
    "X_features = sp.hstack((X_tfidf, X_sentiment), format='csr')  # Concaténer les matrices creuses\n",
    "\n",
    "# Standardiser les colonnes numériques\n",
    "scaler = StandardScaler()  # Initialiser le standardiseur\n",
    "X_numerical = scaler.fit_transform(df[num_cols])  # Standardiser les caractéristiques numériques\n",
    "\n",
    "# Spécifier les poids pour chaque colonne numérique\n",
    "num_feature_weights = {\n",
    "    \"numVotes\": 1,\n",
    "    \"startYear\": 1,\n",
    "    \"runtimeMinutes\": 1,\n",
    "    \"popularity\": 2,\n",
    "}\n",
    "\n",
    "# Appliquer la pondération aux caractéristiques numériques en utilisant les poids spécifiés\n",
    "X_numerical_weighted = X_numerical.copy()  # Copie des données numériques\n",
    "for col, weight in num_feature_weights.items():\n",
    "    col_index = num_cols.index(col)  # Récupérer l'index de la colonne\n",
    "    X_numerical_weighted[:, col_index] *= weight  # Appliquer la pondération\n",
    "\n",
    "# Créer un vecteur de poids pour les caractéristiques catégorielles\n",
    "cat_feature_weights = {\n",
    "    \"title\": 2,\n",
    "    \"genre1\": 3,\n",
    "    \"genre2\": 1,\n",
    "    \"production_companies_name\": 2,\n",
    "    \"Director_name\": 1,\n",
    "    \"Actors_Actresses\": 2,\n",
    "    \"processed_overview\": 2\n",
    "}\n",
    "\n",
    "# Créer un encodeur one-hot\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')  # Ignorer les catégories inconnues\n",
    "\n",
    "# Adapter l'encodeur aux données d'entraînement et le transformer\n",
    "X_cat_encoded = encoder.fit_transform(df[cat_cols])\n",
    "\n",
    "# Appliquer la pondération aux caractéristiques catégorielles en utilisant les poids spécifiés\n",
    "for col, weight in cat_feature_weights.items():\n",
    "    col_index = cat_cols.index(col)  # Récupérer l'index de la colonne\n",
    "    X_cat_encoded[:, col_index] *= weight  # Appliquer la pondération\n",
    "\n",
    "# Concaténer les caractéristiques encodées et standardisées\n",
    "X = sp.hstack((X_features, X_numerical_weighted, X_cat_encoded), format='csr')  # Concaténer les matrices creuses\n",
    "\n",
    "# Calculer la similarité cosinus entre les films\n",
    "cosine_sim = cosine_similarity(X, X)\n",
    "\n",
    "# Fonction de recommandation de films similaires\n",
    "def recommend_movies(movie_title, cosine_sim=cosine_sim, df=df):\n",
    "    idx = df[df['title'] == movie_title].index[0]  # Obtenir l'index du film\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))  # Obtenir les scores de similarité\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)  # Trier par score de similarité\n",
    "    sim_scores = sim_scores[1:6]  # Exclure le film lui-même\n",
    "    movie_indices = [i[0] for i in sim_scores]  # Obtenir les indices des films recommandés\n",
    "    return df['title'].iloc[movie_indices]  # Retourner les titres des films recommandés\n",
    "\n",
    "# Exemple d'utilisation : obtenir des recommandations pour un film spécifique\n",
    "movie_title = \"Le Roi Lion\"\n",
    "recommended_movies = recommend_movies(movie_title)\n",
    "print(\"Recommandations pour\", movie_title, \":\")\n",
    "recommended_movies_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
